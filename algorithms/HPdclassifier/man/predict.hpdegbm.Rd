\name{predict.hpdegbm}
\alias{predict.hpdegbm}
\title{Data-Distributed and Model-Centralized Predict Method for Distributed, Ensemble GBM}
\description{
  This function can be used to apply a model of type \code{hpdegbm} to new data for prediction.
}
\usage{
predict.hpdegbm <- function(model, newdata, trace = FALSE)
}
\arguments{
  \item{model}{a model returned by the function \code{hpdegbm}.}
  \item{newdata}{a darray, dframe, data.frame, or a matrix that contains new data on which to perform predictions. The features of newdata should correspond to those used to train the model, but if not, extra features will be ignored. Hewlett-Packard recommends darray over dframe when there is no categorical data.}
  \item{trace}{when this argument is true, intermediate steps of the progress are displayed.}
}

\value{
  Returns predicted classes or regression values in distributed or non-distributed objects, 
    depending on the type of the input. When the new data is of type darray,
  the type of returned value is also darray, unless the output is categorical data.  The output is a dframe when the new data is of type dframe. The 'model' parameter is expected to be a model generated by the \code{hpdegbm} function. Such a model is in fact an ensemble of \code{gbm} models trained using the \code{gbm} package. In order to make a prediction with the ensemble, each model makes an independent prediction using \code{predict.gbm} and the resulting predictions are
  merged according to the type of the response. In the case of a continuous response variable (as with regression), the predictions of the models in the ensemble are currently averaged to get the final prediction. In all other cases, a voting scheme is used to choose the most common prediction among the models of the ensemble. 
}

\references{
  Package 'gbm' version 2.1.1 \url{http://cran.r-project.org/web/packages/gbm/gbm.pdf}.
}

\author{
    HP Vertica Analytics Team
}


\examples{
 \dontrun{
library(distributedR)
library(HPdclassifier)
distributedR_start()

data(iris)
irisX       <- iris[which(names(iris) != "Species")]
irisY       <- as.character(iris$Species)
trainPerc   <- 0.8
trainIdx    <- sample(1:nrow(irisX), ceiling(trainPerc * nrow(irisX)))
irisX_train <- irisX[trainIdx,]
irisY_train <- irisY[trainIdx]
irisX_test  <- irisX[-trainIdx,]
irisY_test  <- irisY[-trainIdx]

# Create distributed versions of the training data
dirisX_train <- as.dframe(irisX_train)
dirisY_train <- as.dframe(as.data.frame(irisY_train))
dirisX_test  <- as.dframe(irisX_test)
dirisY_test  <- as.dframe(as.data.frame(irisY_test))

# Using centralized data structures. Will build 6 different models in parallel
# on the whole dataset
mod <- hpdegbm(irisX_train, irisY_train, distribution = 'multinomial',
               nExecutor = 6)
# Make predictions using the ensemble
pred <- predict(mod, irisX_test)

# Using dobjects. Will sample the training data into 6 partitions and then build 
# a different model on each partition in parallel
dmod <- hpdegbm(dirisX_train, dirisY_train, distribution = 'multinomial',
                nExecutor = 6)
# Make predictions using the ensemble
dpred <- predict(dmod, irisX_test)

distributedR_shutdown()
}
}

\keyword{distributed GBM}
\keyword{classification}
\keyword{regression}
